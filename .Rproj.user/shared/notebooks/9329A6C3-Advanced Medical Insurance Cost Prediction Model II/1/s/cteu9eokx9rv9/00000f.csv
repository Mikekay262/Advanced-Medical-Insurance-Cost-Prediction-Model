"0",""
"0","# One-hot encode categorical variables"
"0","dummy_model <- dummyVars("" ~ ."", data = train_data, fullRank = TRUE)"
"0","train_data_encoded <- data.frame(predict(dummy_model, newdata = train_data))"
"0","test_data_encoded <- data.frame(predict(dummy_model, newdata = test_data))"
"0",""
"0","# Ensure all columns are numeric"
"0","train_data_encoded <- train_data_encoded %>% mutate(across(everything(), as.numeric))"
"0","test_data_encoded <- test_data_encoded %>% mutate(across(everything(), as.numeric))"
"0",""
"0","# Create xgb.DMatrix objects"
"0","xgb_train <- xgb.DMatrix(data = as.matrix(train_data_encoded), label = train_data$charges)"
"0","xgb_test <- xgb.DMatrix(data = as.matrix(test_data_encoded), label = test_data$charges)"
"0",""
"0","# Validate Data Consistency"
"0","if (nrow(as.matrix(train_data_encoded)) != length(train_data$charges)) {"
"0","  stop(""Mismatch between features and labels in train_data."")"
"0","}"
"0","if (nrow(as.matrix(test_data_encoded)) != length(test_data$charges)) {"
"0","  stop(""Mismatch between features and labels in test_data."")"
"0","}"
"0",""
"0","# 2. Bayesian Optimization Function for XGBoost"
"0","xgb_bayesian <- function(max_depth, eta, subsample, colsample_bytree) {"
"0","  params <- list("
"0","    objective = ""reg:squarederror"","
"0","    max_depth = as.integer(max_depth),"
"0","    eta = eta,"
"0","    subsample = subsample,"
"0","    colsample_bytree = colsample_bytree,"
"0","    nthread = 4  # Enable parallel processing"
"0","  )"
"0","  "
"0","  # Cross-validation"
"0","  xgb_cv <- xgb.cv("
"0","    params = params,"
"0","    data = xgb_train,"
"0","    nrounds = 500,  # Reasonable max boosting rounds"
"0","    nfold = 3,      # Reduce folds for faster evaluation"
"0","    metrics = ""rmse"","
"0","    early_stopping_rounds = 20,"
"0","    verbose = 0"
"0","  )"
"0","  "
"0","  # Handle edge case for missing best_iteration"
"0","  best_nrounds <- ifelse(is.null(xgb_cv$best_iteration), 500, xgb_cv$best_iteration)"
"0","  "
"0","  # Train the model with the best parameters"
"0","  model <- xgb.train("
"0","    params = params,"
"0","    data = xgb_train,"
"0","    nrounds = best_nrounds,"
"0","    verbose = 0"
"0","  )"
"0","  "
"0","  # Predictions and RMSE"
"0","  predictions <- predict(model, xgb_test)"
"0","  rmse <- sqrt(mean((test_data$charges - predictions)^2))"
"0","  "
"0","  list(Score = -rmse, Pred = model, Nrounds = best_nrounds)"
"0","}"
"0",""
"0","# 3. Perform Bayesian Optimization"
"0","set.seed(42)"
"0","xgb_bo <- BayesianOptimization("
"0","  FUN = xgb_bayesian,"
"0","  bounds = list("
"0","    max_depth = c(3L, 10L),"
"0","    eta = c(0.01, 0.3),"
"0","    subsample = c(0.7, 1),"
"0","    colsample_bytree = c(0.7, 1)"
"0","  ),"
"0","  init_points = 5,"
"0","  n_iter = 20,"
"0","  acq = ""ucb"""
"0",")"
"1","elapsed = 8.16	Round = 1	max_depth = 9.0000	eta = 0.1605378	subsample = 0.8373225	colsample_bytree = 0.9820044	Value = -154.6936"
"1"," "
"1","
"
"1","elapsed = 3.39	Round = 2	max_depth = 10.0000	eta = 0.2236106	subsample = 0.9157337	colsample_bytree = 0.9934679	Value = -228.0648"
"1"," "
"1","
"
"1","elapsed = 6.14	Round = 3	max_depth = 5.0000	eta = 0.04905331	subsample = 0.9804017	colsample_bytree = 0.7352462	Value = -591.5660"
"1"," "
"1","
"
"1","elapsed = 3.18	Round = 4	max_depth = 9.0000	eta = 0.2005278	subsample = 0.7766286	colsample_bytree = 0.8424991	Value = -377.7979"
"1"," "
"1","
"
"1","elapsed = 7.66	Round = 5	max_depth = 7.0000	eta = 0.2144688	subsample = 0.8386878	colsample_bytree = 0.8680998	Value = -489.4881"
"1"," "
"1","
"
"1","elapsed = 7.77	Round = 6	max_depth = 9.0000	eta = 0.0100	subsample = 0.9909259	colsample_bytree = 0.9445517	Value = -447.6260"
"1"," "
"1","
"
"1","elapsed = 2.21	Round = 7	max_depth = 3.0000	eta = 0.3000	subsample = 0.7000	colsample_bytree = 1.0000	Value = -381.5564"
"1"," "
"1","
"
"1","elapsed = 8.99	Round = 8	max_depth = 9.0000	eta = 0.06766773	subsample = 0.813348	colsample_bytree = 1.0000	Value = -115.5309"
"1"," "
"1","
"
"1","elapsed = 1.80	Round = 9	max_depth = 10.0000	eta = 0.2517071	subsample = 0.7000	colsample_bytree = 0.9848041	Value = -287.5922"
"1"," "
"1","
"
"1","elapsed = 2.72	Round = 10	max_depth = 10.0000	eta = 0.3000	subsample = 0.9932048	colsample_bytree = 0.7874764	Value = -1325.5548"
"1"," "
"1","
"
"1","elapsed = 1.89	Round = 11	max_depth = 9.0000	eta = 0.3000	subsample = 0.7000	colsample_bytree = 0.7000	Value = -669.0375"
"1"," "
"1","
"
"1","elapsed = 3.77	Round = 12	max_depth = 3.0000	eta = 0.0100	subsample = 0.7000	colsample_bytree = 0.7000	Value = -1271.0771"
"1"," "
"1","
"
"1","elapsed = 1.93	Round = 13	max_depth = 9.0000	eta = 0.3000	subsample = 0.9838869	colsample_bytree = 0.9178851	Value = -224.9772"
"1"," "
"1","
"
"1","elapsed = 3.65	Round = 14	max_depth = 3.0000	eta = 0.0100	subsample = 0.91239	colsample_bytree = 0.9035874	Value = -789.2112"
"1"," "
"1","
"
"1","elapsed = 4.68	Round = 15	max_depth = 5.0000	eta = 0.3000	subsample = 0.9349806	colsample_bytree = 0.9428715	Value = -408.3555"
"1"," "
"1","
"
"1","elapsed = 6.60	Round = 16	max_depth = 7.0000	eta = 0.0100	subsample = 0.764681	colsample_bytree = 0.8401223	Value = -657.1144"
"1"," "
"1","
"
"1","elapsed = 5.61	Round = 17	max_depth = 6.0000	eta = 0.0100	subsample = 0.935249	colsample_bytree = 0.9840084	Value = -420.3384"
"1"," "
"1","
"
"1","elapsed = 8.81	Round = 18	max_depth = 10.0000	eta = 0.0100	subsample = 0.9794313	colsample_bytree = 0.8804068	Value = -645.3962"
"1"," "
"1","
"
"1","elapsed = 3.33	Round = 19	max_depth = 10.0000	eta = 0.3000	subsample = 0.7000	colsample_bytree = 0.893692	Value = -644.3185"
"1"," "
"1","
"
"1","elapsed = 1.88	Round = 20	max_depth = 8.0000	eta = 0.3000	subsample = 0.9905041	colsample_bytree = 1.0000	Value = -247.0145"
"1"," "
"1","
"
"1","elapsed = 4.27	Round = 21	max_depth = 7.0000	eta = 0.2894137	subsample = 0.9933751	colsample_bytree = 0.9129171	Value = -245.2481"
"1"," "
"1","
"
"1","elapsed = 3.20	Round = 22	max_depth = 3.0000	eta = 0.0100	subsample = 0.7000	colsample_bytree = 0.9769014	Value = -526.1476"
"1"," "
"1","
"
"1","elapsed = 1.97	Round = 23	max_depth = 3.0000	eta = 0.3000	subsample = 0.7000	colsample_bytree = 0.7929499	Value = -562.9228"
"1"," "
"1","
"
"1","elapsed = 4.69	Round = 24	max_depth = 10.0000	eta = 0.2146027	subsample = 0.7881165	colsample_bytree = 1.0000	Value = -234.0391"
"1"," "
"1","
"
"1","elapsed = 1.79	Round = 25	max_depth = 7.0000	eta = 0.297981	subsample = 0.8150998	colsample_bytree = 0.9463932	Value = -706.6844"
"1"," "
"1","
"
"1","
 Best Parameters Found: 
"
"1","Round = 8	max_depth = 9.0000	eta = 0.06766773	subsample = 0.813348	colsample_bytree = 1.0000	Value = -115.5309"
"1"," "
"1","
"
"0","# Retrieve Optimal Parameters"
"0","best_params <- xgb_bo$Best_Par"
"0","best_nrounds <- ifelse("
"0","  is.null(xgb_bo$History$Nrounds[[which.min(xgb_bo$History$Value)]]),"
"0","  500,"
"0","  xgb_bo$History$Nrounds[[which.min(xgb_bo$History$Value)]]"
"0",")"
"0",""
"0","# 4. Train Final Model with Optimal Parameters"
"0","xgb_best <- xgb.train("
"0","  params = list("
"0","    objective = ""reg:squarederror"","
"0","    max_depth = best_params[""max_depth""],"
"0","    eta = best_params[""eta""],"
"0","    subsample = best_params[""subsample""],"
"0","    colsample_bytree = best_params[""colsample_bytree""]"
"0","  ),"
"0","  data = xgb_train,"
"0","  nrounds = best_nrounds,"
"0","  verbose = 0"
"0",")"
"0",""
"0","# 5. Evaluate the Final Model"
"0","xgb_predictions <- predict(xgb_best, xgb_test)"
"0","xgb_rmse <- sqrt(mean((test_data$charges - xgb_predictions)^2))"
"0","cat(""Optimized XGBoost RMSE:"", xgb_rmse, ""\n"")"
"1","Optimized XGBoost RMSE:"
"1"," "
"1","76.42326"
"1"," "
"1","
"
